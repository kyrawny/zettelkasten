<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>1Permanents on</title><link>https://kyrawny.github.io/1permanent/</link><description>Recent content in 1Permanents on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 22 Aug 1970 22:06:49 +0000</lastBuildDate><atom:link href="https://kyrawny.github.io/1permanent/index.xml" rel="self" type="application/rss+xml"/><item><title>202108092218 SNNs use less arithmetic operations for certain sparse coding optimization problems</title><link>https://kyrawny.github.io/1Permanent/202108092218-SNNs-use-less-arithmetic-operations-for-certain-sparse-coding-optimization-problems/</link><pubDate>Sat, 22 Aug 1970 22:06:49 +0000</pubDate><guid>https://kyrawny.github.io/1Permanent/202108092218-SNNs-use-less-arithmetic-operations-for-certain-sparse-coding-optimization-problems/</guid><description>Contexts Neuromorphic Computing | Spiking Neural Networks | Statistics | Machine Learning | Regression Analysis | Representation Learning | Optimization | Unsupervised Learning
202108092218 SNNs use less arithmetic operations for certain sparse coding optimization problems Sparse coding refers to unsupervised methods of learning how to represent data efficiently using a linear combination of a relatively small number of elements from a feature dictionary. One such method is the Least Absolute Shrinkage and Selection Operator (LASSO), which finds an estimate of the representation by minimizing the least square error subject to a â„“1-norm constraint in the solution vector.</description></item><item><title>202108041944 SNNs utilize spatiotemporal data, unlike DNNs</title><link>https://kyrawny.github.io/1Permanent/202108041944-SNNs-utilize-spatiotemporal-data-unlike-DNNs/</link><pubDate>Sat, 22 Aug 1970 22:06:44 +0000</pubDate><guid>https://kyrawny.github.io/1Permanent/202108041944-SNNs-utilize-spatiotemporal-data-unlike-DNNs/</guid><description>Contexts Neuromorphic Computing | Spiking Neural Networks | Artificial Intelligence | Machine Learning
202108041944 SNNs utilize spatiotemporal data, unlike DNNs SNNs incorporate the concept of time and space in their operating model. Unlike DNNs, where every neuron sends a continuous output at every time cycle to every neuron in the next layer, neurons in SNNs only transmit single-bit impulses (or spikes) to neighbors through directed connections known as synapses, when a membrane potential reaches a certain threshold.</description></item><item><title>202107291348 Multiply accumulate arrays efficiently perform matrix multiplications in DNNs</title><link>https://kyrawny.github.io/1Permanent/202107291348-Multiply-accumulate-arrays-efficiently-perform-matrix-multiplications-in-DNNs/</link><pubDate>Sat, 22 Aug 1970 22:05:29 +0000</pubDate><guid>https://kyrawny.github.io/1Permanent/202107291348-Multiply-accumulate-arrays-efficiently-perform-matrix-multiplications-in-DNNs/</guid><description>Contexts Deep Neural Networks | Computing Hardware | Artificial Intelligence | Machine Learning
202107291348 Multiply accumulate arrays efficiently perform matrix multiplications in DNNs Every connection between neurons in a deep neural network is assigned a weight. Inputs to the deep neural network are multiplied by these weights. Thus, in the forward pass of a deep neural network, much of the computation done consists of vector matrix multiplications. In computing, these calculations are achieved through the accumulation of products, via multiply-accumulate operations.</description></item><item><title>202107291603 MAC arrays, a tool for DNNs, can be used to supplement the performance of SNNs with high-dimensional inputs, while maintaining system flexibility</title><link>https://kyrawny.github.io/1Permanent/202107291603-MAC-arrays-a-tool-for-DNNs-can-be-used-to-supplement-the-performance-of-SNNs-with-high-dimensional-inputs-while-maintaining-system-flexibility/</link><pubDate>Sat, 22 Aug 1970 22:05:29 +0000</pubDate><guid>https://kyrawny.github.io/1Permanent/202107291603-MAC-arrays-a-tool-for-DNNs-can-be-used-to-supplement-the-performance-of-SNNs-with-high-dimensional-inputs-while-maintaining-system-flexibility/</guid><description>Contexts Neuromorphic Computing | Computing Hardware | Deep Neural Networks | Spiking Neural Networks | Artificial Intelligence | Machine Learning
202107291603 MAC arrays, a tool for DNNs, can be used to supplement the performance of SNNs with high-dimensional inputs, while maintaining system flexibility Neuromorphic hardware platforms like Loihi have dedicated circuits for synapses and neurons to increase the efficiency of spiking neural network models implemented on them. However, this results in less flexible systems that cannot be used to implement more diverse models, that additionally do not have specialized tools to handle specific types of computations.</description></item></channel></rss>